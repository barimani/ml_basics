{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d12f674",
   "metadata": {},
   "source": [
    "# Dataset: Boston housing\n",
    "\n",
    "- CRIM - per capita crime rate by town\n",
    "- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS - proportion of non-retail business acres per town.\n",
    "- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "- NOX - nitric oxides concentration (parts per 10 million)\n",
    "- RM - average number of rooms per dwelling\n",
    "- AGE - proportion of owner-occupied units built prior to 1940\n",
    "- DIS - weighted distances to five Boston employment centres\n",
    "- RAD - index of accessibility to radial highways\n",
    "- TAX - full-valjue property-tax rate per $10,000\n",
    "\n",
    "- PTRATIO - pupil-teacher ratio by town\n",
    "- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- LSTAT - % lower status of the population\n",
    "- MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e27a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43faa749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f08f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find a feature that correlates strongly\n",
    "df.corr()['medv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634be02f",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "\n",
    "#### Data\n",
    "Feature or input variable or independent variable\n",
    "Target or dependent variable or prediction\n",
    "\n",
    "#### Modelling\n",
    "model - a function or a system or sequence of logic that takes input and produces an output\n",
    "\n",
    "loss function - a defined function that represents the error of your model compared with real results\n",
    "\n",
    "\n",
    "\n",
    "### Simple case: Linear regression with one input\n",
    "\n",
    "\n",
    "y = b + w X\n",
    "\n",
    "X -> input variable lets take the room number only\n",
    "\n",
    "y -> value of the dwelling\n",
    "\n",
    "The goal is to find w and b that reduces the loss function below:\n",
    "\n",
    "loss = Sum( ( y - yPred ) ^ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's focus only on a single feature as opposed to all features\n",
    "X = df[['rm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87daee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are trying to predict the price, so we will use 'medv'\n",
    "y = df['medv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feab3c3",
   "metadata": {},
   "source": [
    "### Train Test split\n",
    "\n",
    "We keep a portion of the data for evaluation of the model. typically between 15 to 25 percent of the data\n",
    "This is to ensure the model is able to make good prediction for unseen data as a complex model can be capable of memorizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7d123",
   "metadata": {},
   "source": [
    "### Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_train, y_train, label='Training set')\n",
    "plt.scatter(X_test, y_test, label='Test set')\n",
    "\n",
    "plt.xlabel('Feature (average number of rooms)')\n",
    "plt.ylabel('Target (median value)')\n",
    "\n",
    "plt.title('Real Estate Price based on number of rooms')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b37b19",
   "metadata": {},
   "source": [
    "### Training\n",
    "Is based on the train set only and is simply a single line (in general is a hyperplane)\n",
    "\n",
    "There is so much action happening under the 'fit' method here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7747e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model coefficients (parameters)\n",
    "b = reg.intercept_\n",
    "w = reg.coef_\n",
    "\n",
    "print(f'b={b} and w={w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136149a",
   "metadata": {},
   "source": [
    "### Evaluating\n",
    "We will evaluate the performance of the model for both training and test datasets\n",
    "\n",
    "The score of a linear regression is the coefficient of determination which is 1 - loss / (variance of true targets * size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = reg.score(X_train, y_train)\n",
    "test_score = reg.score(X_test, y_test)\n",
    "\n",
    "print(f'Train score is: {train_score}')\n",
    "print(f'Test score is: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction -> the model can be used for the application\n",
    "num_rooms = 6.\n",
    "price = reg.predict([[num_rooms]])\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34037313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the fitted line\n",
    "y_pred = reg.predict(X)\n",
    "\n",
    "plt.scatter(X['rm'], y, label='All data')\n",
    "plt.scatter(X['rm'], y_pred, color='red', label='Fitted line')\n",
    "\n",
    "\n",
    "plt.xlabel('Feature (average number of rooms)')\n",
    "plt.ylabel('Target (median value)')\n",
    "\n",
    "plt.title('Real Estate Price based on number of rooms')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a58025",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Why did we get such scores?\n",
    "\n",
    "Could it be that our model is too simple?\n",
    "No based on the scatter plot it seems like a more complex can't do better necessarily\n",
    "\n",
    "Could it be an isssue with the data? Yes, it seems like that simply relying on the number of rooms is not a good criteria for predicting the price. Let's include more features and see if we can get a better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "X = df.drop(columns=['medv'])\n",
    "y = df['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c66bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling: we need to bring all the features in the same scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Using a scaler object to bring all features in the same range (only using the training set to avoid data leak)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Generating a scaled train and test set\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the scaled values in a DataFrame format\n",
    "pd.DataFrame(X_train_scaled, columns=[c + '_scaled' for c in X_train.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "reg_full = linear_model.LinearRegression()\n",
    "\n",
    "reg_full.fit(X_train_scaled, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801212f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = reg_full.score(X_train_scaled, y_train)\n",
    "test_score = reg_full.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f'Train score is: {train_score}')\n",
    "print(f'Test score is: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73099a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For prediction you have to provide all the features now.\n",
    "# Lets calulatethe prediction for all test data by adding a column and comparing\n",
    "y_test_predictions= reg_full.predict(X_test_scaled)\n",
    "\n",
    "y_test_predictions_series = pd.Series(y_test_predictions, name='medv_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([y_test.reset_index(drop=True), y_test_predictions_series], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7c6fd",
   "metadata": {},
   "source": [
    "# Bias-Variance trade off\n",
    "\n",
    "A key concept in ML to understand is bias-variance trade-off. \n",
    "\n",
    "- A model with high bias makes strong assumptions such as a linear relationship\n",
    "- A model with high variance makes minimal assumptions and conforms to variations in data\n",
    "\n",
    "high bias and high variance both can hurt predictions. There is always a trade-off meaning you cannot have a model with low bias and low variance. Finding the right balance is key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511faa6",
   "metadata": {},
   "source": [
    "### kNN\n",
    "K Nearest Neighbors is a low bias model. It simply memorizes the whole training set. It will then look at the k closest data points in the training set and makes a prediction based on those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ff09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "X = df[['rm']]\n",
    "y = df['medv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff82699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# the Hyper parameter k acts as a 'regularization'. The higher k the lower the variance\n",
    "k = 1\n",
    "\n",
    "knnRegressor = KNeighborsRegressor(n_neighbors=k)\n",
    "knnRegressor.fit(X_train, y_train);\n",
    "\n",
    "\n",
    "# Let's calculate the score\n",
    "score = knnRegressor.score(X_test, y_test)\n",
    "\n",
    "import numpy as np\n",
    "sample = pd.DataFrame([[rm] for rm in np.arange(3,9, 0.001)], columns=['rm'])\n",
    "\n",
    "plt.scatter(X_train, y_train, label='training set')\n",
    "plt.plot(sample, knnRegressor.predict(sample), color='red', label=f'knn prediction with k = {k}')\n",
    "\n",
    "plt.xlabel('Number of rooms')\n",
    "plt.ylabel('Predicted price')\n",
    "plt.title(f'Score={score}')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e01d8",
   "metadata": {},
   "source": [
    "### Simple Hyperparameter tuning\n",
    "\n",
    "Involves looking for the best hyperparameter configuration for maximizing a score.\n",
    "\n",
    "In our kNN case, we want to find the best K for maximum score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(k):\n",
    "    knnRegressor = KNeighborsRegressor(n_neighbors=k)\n",
    "    knnRegressor.fit(X_train, y_train)\n",
    "    \n",
    "    trainScore = knnRegressor.score(X_train, y_train)\n",
    "    testScore = knnRegressor.score(X_test, y_test)\n",
    "    \n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1,100)\n",
    "scores = [getScores(k) for k in ks]\n",
    "\n",
    "train_scores = [score[0] for score in scores]\n",
    "test_scores = [score[1] for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca449612",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(ks, train_scores, label='Train score')\n",
    "plt.scatter(ks, test_scores, label='Test score')\n",
    "plt.xlabel('Hyperparameter K')\n",
    "plt.ylabel('kNN score')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750cc17",
   "metadata": {},
   "source": [
    "### Knn with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "X = df.drop(columns=['medv'])\n",
    "y = df['medv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "\n",
    "# Scaling here is particularly important due to the notion of 'distance' between neighbors\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Rerun cells above make sure to user the scaled version of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57892bcc",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "An important technique to ensure your analysis is not sensitive to your data split. In the example above let's change the random_state to produce different splits. You will see that the scores chart will change drammatically.\n",
    "\n",
    "![Cross Validation](https://zitaoshen.rbind.io/project/machine_learning/machine-learning-101-cross-vaildation/featured.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb82b0b",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "As opposed to linear regression that assumes target is a linear combination of feature values, polynomial regression assumes a more comples shape. For example a polynomal of degree 2 with a single feature is simply a parabola:\n",
    "\n",
    "y = b + w0 X + w1 X ^ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04122c8",
   "metadata": {},
   "source": [
    "To achieve the form above, we can simply consider X^2 as a new feature and convert our problem to a LinearRegression with two features. The goal is to find the best b, w0, and w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column for squared values of rmsss\n",
    "X.insert(1, 'rm squared', df['rm']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b42f93",
   "metadata": {},
   "source": [
    "### Exercise: fit a polynomial of degree 3 to the one dimentional training set and plot the curve\n",
    "\n",
    "y = b + w0 X + w1 X ^ 2 + w2 X ^ 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
